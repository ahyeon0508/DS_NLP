# ğŸ“– ì¶œì²˜: í…ì„œí”Œë¡œ 2ì™€ ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬
# ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ê°€ê³µí•˜ëŠ” íŒŒì¼

import os
import re
import json

import numpy as np
import pandas as pd
from tqdm import tqdm

from konlpy.tag import Okt

FILTERS = "([~.,!?\"':;)(])"
# ì–´ë–¤ ì˜ë¯¸ë„ ì—†ëŠ” íŒ¨ë”© í† í°
PAD = "<PAD>"
# ì‹œì‘ í† í°
STD = "<SOS>"
# ì¢…ë£Œ í† í°
END = "<END>"
# ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´
UNK = "<UNK>"

PAD_INDEX = 0
STD_INDEX = 1
END_INDEX = 2
UNK_INDEX = 3

MARKER = [PAD, STD, END, UNK]
CHANGE_FILTER = re.compile(FILTERS)

MAX_SEQUENCE = 25

# ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def load_data(path):
    data_df = pd.read_csv(path, header=0)
    question, answer = list(data_df['Q']), list(data_df['A'])

    return question, answer

# ë°ì´í„° ì „ì²˜ë¦¬ í•œ í›„ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜
def data_tokenizer(data):
    words = []
    for sentence in data:
        sentence = re.sub(CHANGE_FILTER, "", sentence)
        for word in sentence.split():
            words.append(word)
    return [word for word in words if word]

# í˜•íƒœì†Œë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
def prepro_like_morphlized(data):
    morph_analyzer = Okt()
    result_data = list()
    for seq in tqdm(data):
        morphlized_seq = " ".join(morph_analyzer.morphs(seq.replace(' ', '')))
        result_data.append(morphlized_seq)

    return result_data

# ë‹¨ì–´ ì‚¬ì „ ë§Œë“œëŠ” í•¨ìˆ˜
def load_vocabulary(path, vocab_path, tokenize_as_morph=False):
    vocabulary_list = []
    if not os.path.exists(vocab_path):
        if (os.path.exists(path)):
            data_df = pd.read_csv(path, encoding='utf-8')
            question, answer = list(data_df['Q']), list(data_df['A'])

            if tokenize_as_morph:  # í˜•íƒœì†Œì— ë”°ë¥¸ í† í¬ë‚˜ì´ì ¸ ì²˜ë¦¬
                question = prepro_like_morphlized(question)
                answer = prepro_like_morphlized(answer)

            data = []
            data.extend(question) # extend() : ë¦¬ìŠ¤íŠ¸ ëì— ê°€ì¥ ë°”ê¹¥ìª½ iterableì˜ ëª¨ë“  í•­ëª©ì„ ë„£ìŒ
            data.extend(answer)
            words = data_tokenizer(data)
            words = list(set(words)) # ê³µí†µ ë‹¨ì–´ ì œê±°
            words[:0] = MARKER # words ë¦¬ìŠ¤íŠ¸ ì²˜ìŒì— MARKER ë°ì´í„° ë„£ìŒ

        # ì‚¬ì „ì— ë°ì´í„° ì‚¬ì „ ì¡´ì¬x
        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:
            for word in words:
                vocabulary_file.write(word + '\n')

    # ì‚¬ì „ì— ë°ì´í„° ì‚¬ì „ ì¡´ì¬
    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:
        for line in vocabulary_file:
            vocabulary_list.append(line.strip())

    char2idx, idx2char = make_vocabulary(vocabulary_list)
    return char2idx, idx2char, len(char2idx)

def make_vocabulary(vocabulary_list):
    # í‚¤ê°€ ë‹¨ì–´ì´ê³  ê°’ì´ ì¸ë±ìŠ¤ì¸ ë”•ì…”ë„ˆë¦¬
    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}
    # í‚¤ê°€ ì¸ë±ìŠ¤ì´ê³  ê°’ì´ ë‹¨ì–´ì¸ ë”•ì…”ë„ˆë¦¬
    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}
    return char2idx, idx2char

# ì¸ì½”ë” ì „ì²˜ë¦¬ í•¨ìˆ˜
def enc_processing(value, dictionary, tokenize_as_morph=False):
    sequences_input_index = []
    sequences_length = []

    if tokenize_as_morph:
        value = prepro_like_morphlized(value)

    for sequence in value:
        # 1. íŠ¹ìˆ˜ë¬¸ì ëª¨ë‘ ì œê±°
        sequence = re.sub(CHANGE_FILTER, "", sequence)
        sequence_index = []

        # 2. ì˜ë ¤ì§„ ë‹¨ì–´ë¥¼ ë‹¨ì–´ ì‚¬ì „ì„ ì´ìš©í•´ ë‹¨ì–´ ì¸ë±ìŠ¤ë¡œ ë°”ê¿ˆ
        for word in sequence.split():
            if dictionary.get(word) is not None:
                sequence_index.extend([dictionary[word]])
            # ì˜ë ¤ì§„ ë‹¨ì–´ê°€ ë”•ì…”ë„ˆë¦¬ì— ì¡´ì¬ í•˜ì§€ ì•ŠëŠ” ê²½ìš° UNK(2)ë¥¼ ë„£ì–´ ì¤€ë‹¤.
            else:
                sequence_index.extend([dictionary[UNK]])

        # 3. ë¬¸ì¥ ì œí•œ ê¸¸ì´ë³´ë‹¤ ê¸¸ì–´ì§ˆ ê²½ìš° ë’¤ë¥¼ ìë¦„
        if len(sequence_index) > MAX_SEQUENCE:
            sequence_index = sequence_index[:MAX_SEQUENCE]

        # í•˜ë‚˜ì˜ ë¬¸ì¥ì— ëŒ€í•œ ê¸¸ì´
        sequences_length.append(len(sequence_index))
        # max_sequence_lengthë³´ë‹¤ ë¬¸ì¥ ê¸¸ì´ê°€ ì‘ì„ ê²½ìš° ë¹ˆ ë¶€ë¶„ì— PAD(0)ë¥¼ ë„£ì–´ì¤Œ
        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]
        # ì¸ë±ìŠ¤í™” ë˜ì–´ ìˆëŠ” ê°’ì„ sequences_input_indexì— ë„£ì–´ì¤Œ
        sequences_input_index.append(sequence_index)

    # ì¸ë±ìŠ¤í™”ëœ ì¼ë°˜ ë°°ì—´ì„ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€ê²½í•¨ : í…ì„œí”Œë¡œìš° datasetì— ë„£ì–´ ì£¼ê¸° ìœ„í•´
    return np.asarray(sequences_input_index), sequences_length

# ë””ì½”ë” ì „ì²˜ë¦¬ í•¨ìˆ˜
def dec_output_processing(value, dictionary, tokenize_as_morph=False):
    sequences_output_index = []
    sequences_length = []

    if tokenize_as_morph:
        value = prepro_like_morphlized(value)

    for sequence in value:
        sequence = re.sub(CHANGE_FILTER, "", sequence)
        # ë””ì½”ë”© ì…ë ¥ì˜ ì²˜ìŒì—ëŠ” STARTê°€ ì™€ì•¼í•¨
        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]
        # ë¬¸ì¥ ì œí•œ ê¸¸ì´ë³´ë‹¤ ê¸¸ì–´ì§ˆ ê²½ìš° ë’¤ë¥¼ ìë¦„
        if len(sequence_index) > MAX_SEQUENCE:
            sequence_index = sequence_index[:MAX_SEQUENCE]

        # í•˜ë‚˜ì˜ ë¬¸ì¥ì— ëŒ€í•œ ê¸¸ì´
        sequences_length.append(len(sequence_index))
        # max_sequence_lengthë³´ë‹¤ ë¬¸ì¥ ê¸¸ì´ê°€ ì‘ì„ ê²½ìš° ë¹ˆ ë¶€ë¶„ì— PAD(0)ë¥¼ ë„£ì–´ì¤Œ
        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]
        # ì¸ë±ìŠ¤í™” ë˜ì–´ ìˆëŠ” ê°’ì„ sequences_input_indexì— ë„£ì–´ì¤Œ
        sequences_output_index.append(sequence_index)
    # ì¸ë±ìŠ¤í™”ëœ ì¼ë°˜ ë°°ì—´ì„ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€ê²½í•¨ : í…ì„œí”Œë¡œìš° datasetì— ë„£ì–´ ì£¼ê¸° ìœ„í•´
    return np.asarray(sequences_output_index), sequences_length

# ë””ì½”ë”ì˜ íƒ€ê¹ƒê°’ì„ ë§Œë“œëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜
# ë””ì½”ë”ì˜ ì…ë ¥ê°’ì„ ë§Œë“œëŠ” í•¨ìˆ˜ì™€ì˜ ì°¨ì´ì ì€ ë¬¸ì¥ì´ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì— ì‹œì‘ í† í°ì„ ë„£ì§€ ì•Šê³  ë§ˆì§€ë§‰ì— ì¢…ë£Œ í† í°ì„ ë„£ëŠ”ë‹¤ëŠ” ì 
def dec_target_processing(value, dictionary, tokenize_as_morph=False):
    sequences_target_index = []
    if tokenize_as_morph:
        value = prepro_like_morphlized(value)

    for sequence in value:
        sequence = re.sub(CHANGE_FILTER, "", sequence)
        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]

        if len(sequence_index) >= MAX_SEQUENCE:
            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]
        else:
            sequence_index += [dictionary[END]]

        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]
        sequences_target_index.append(sequence_index)

    return np.asarray(sequences_target_index)